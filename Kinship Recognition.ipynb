{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib scikit-image tensorflow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_faces_path='data/final-train-faces/'\n",
    "train_faces_excel='data/train-set-pairs.xlsx'\n",
    "\n",
    "validate_faces_path='data/final-validation-faces/'\n",
    "validate_faces_excel='data/val-set-pairs.xlsx'\n",
    "\n",
    "test_faces_path='data/final-test-faces/'\n",
    "test_faces_excel='data/test-set-pairs.xlsx'\n",
    "\n",
    "# possible values: (concatenation, add_subtract, add_subtract_multiply, squared_difference_squared_sum, squared_difference_squared_sum_multiply)\n",
    "fusion_type = 'concatenation'\n",
    "\n",
    "# possible values: (concatenation: 4096, add_subtract: 4096, add_subtract_multiply: 6144, squared_difference_squared_sum: 4096, squared_difference_squared_sum_multiply: 6144)\n",
    "fusion_input_dim = 4096\n",
    "\n",
    "ignore_ptypes = ['gmgs', 'gfgs', 'gfgd', 'gmgd']\n",
    "\n",
    "calculate_features = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_df=pd.read_excel(train_faces_excel)\n",
    "train_df=train_df[['p1','p2','ptype','nsamples']]\n",
    "\n",
    "val_df=pd.read_excel(validate_faces_excel)\n",
    "val_df=val_df[['p1','p2','ptype','nsamples']]\n",
    "\n",
    "\n",
    "test_df=pd.read_excel(test_faces_excel)\n",
    "test_df=test_df[['p1','p2','ptype','nsamples']]\n",
    "\n",
    "print(f'Train size bevore cleanup: {len(train_df)}')\n",
    "print(f'Val size bevore cleanup: {len(val_df)}')\n",
    "print(f'Test size bevore cleanup: {len(test_df)}')\n",
    "\n",
    "# Filter out rows with specified ptype values\n",
    "train_df = train_df[~train_df['ptype'].isin(ignore_ptypes)]\n",
    "val_df = val_df[~val_df['ptype'].isin(ignore_ptypes)]\n",
    "test_df = test_df[~test_df['ptype'].isin(ignore_ptypes)]\n",
    "\n",
    "print(f'Train size after cleanup: {len(train_df)}')\n",
    "print(f'Val size after cleanup: {len(val_df)}')\n",
    "print(f'Test size after cleanup: {len(test_df)}')\n",
    "\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich face image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "def enrich_face_image_pairs(path, df):\n",
    "    new_pair = pd.DataFrame(columns=['p1_path', 'p2_path', 'ptype', 'tag'])\n",
    "    index = 0\n",
    "    tag = 0\n",
    "    for i in tqdm(range(len(df))):\n",
    "        p1 = df.iloc[i]['p1']\n",
    "        p2 = df.iloc[i]['p2']\n",
    "        ptype = df.iloc[i]['ptype']\n",
    "        for p1_path in os.listdir(path + p1):\n",
    "            for p2_path in os.listdir(path + p2):\n",
    "                new_pair.loc[index] = [p1 + '/' + p1_path, p2 + '/' + p2_path, ptype, tag]\n",
    "                index += 1\n",
    "                tag += 1\n",
    "    return new_pair\n",
    "\n",
    "if calculate_features:\n",
    "    train_new_pair = enrich_face_image_pairs(train_faces_path, train_df)\n",
    "    val_new_pair = enrich_face_image_pairs(validate_faces_path, val_df)\n",
    "    test_new_pair = enrich_face_image_pairs(test_faces_path, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    tqdm.pandas(desc='Processing:')\n",
    "    train_new_pair['p1_path']=train_new_pair.progress_apply(lambda x: train_faces_path+x['p1_path'], axis=1)\n",
    "    train_new_pair['p2_path']=train_new_pair.progress_apply(lambda x: train_faces_path+x['p2_path'], axis=1)\n",
    "\n",
    "    val_new_pair['p1_path']=val_new_pair.progress_apply(lambda x: validate_faces_path+x['p1_path'], axis=1)\n",
    "    val_new_pair['p2_path']=val_new_pair.progress_apply(lambda x: validate_faces_path+x['p2_path'], axis=1)\n",
    "\n",
    "    test_new_pair['p1_path']=test_new_pair.progress_apply(lambda x: test_faces_path+x['p1_path'], axis=1)\n",
    "    test_new_pair['p2_path']=test_new_pair.progress_apply(lambda x: test_faces_path+x['p2_path'], axis=1)\n",
    "\n",
    "    print(train_new_pair)\n",
    "    print(val_new_pair)\n",
    "    print(test_new_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    train_df['nsamples'].sum()\n",
    "    val_df['nsamples'].sum()\n",
    "    test_df['nsamples'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    train_new_pair.to_csv(f'{fusion_type}_full_train_pairs.csv')\n",
    "    val_new_pair.to_csv(f'{fusion_type}_full_val_pairs.csv')\n",
    "    test_new_pair.to_csv(f'{fusion_type}_full_test_pairs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "model = ResNet50(weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=input_shape)\n",
    "    return base_model\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "base_network = create_base_network(input_shape)\n",
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_concatenation(x, y):\n",
    "    return np.concatenate((x, y), axis=-1)\n",
    "\n",
    "def feature_add_subtract(x, y):\n",
    "    return np.concatenate((x + y, x - y), axis=-1)\n",
    "\n",
    "def feature_add_subtract_multiply(x, y):\n",
    "    return np.concatenate((x + y, x - y, x * y), axis=-1)\n",
    "\n",
    "def feature_squared_difference_squared_sum(x, y):\n",
    "    return np.concatenate((x**2 - y**2, (x - y)**2), axis=-1)\n",
    "\n",
    "def feature_squared_difference_squared_sum_multiply(x, y):\n",
    "    return np.concatenate((x**2 - y**2, (x - y)**2, x * y), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_feature(p_path, model):\n",
    "    image = imread(p_path)\n",
    "    image_224 = resize(image, (224, 224), preserve_range=True, mode='reflect')\n",
    "    image_224_batch = np.expand_dims(image_224, axis=0)\n",
    "    preprocessed_batch = preprocess_input(image_224_batch)\n",
    "    feature_arr = model.predict(preprocessed_batch)\n",
    "    return feature_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_features(feature_x, feature_y, fusion_type='concatenation'):\n",
    "    if fusion_type == 'concatenation':\n",
    "        return feature_concatenation(feature_x, feature_y)\n",
    "    elif fusion_type == 'add_subtract':\n",
    "        return feature_add_subtract(feature_x, feature_y)\n",
    "    elif fusion_type == 'add_subtract_multiply':\n",
    "        return feature_add_subtract_multiply(feature_x, feature_y)\n",
    "    elif fusion_type == 'squared_difference_squared_sum':\n",
    "        return feature_squared_difference_squared_sum(feature_x, feature_y)\n",
    "    elif fusion_type == 'squared_difference_squared_sum_multiply':\n",
    "        return feature_squared_difference_squared_sum_multiply(feature_x, feature_y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid fusion type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def calculate_similarity(p_path1, p_path2, model, fusion_type='concatenation'):\n",
    "    feature_x = cal_feature(p_path1, model)\n",
    "    feature_y = cal_feature(p_path2, model)\n",
    "    fused_features = fuse_features(feature_x, feature_y, fusion_type=fusion_type)\n",
    "    similarity_score = compute_similarity(fused_features)\n",
    "    return similarity_score\n",
    "\n",
    "def compute_similarity(fused_features):\n",
    "    fc1 = Dense(128, activation='relu')(fused_features)\n",
    "    fc2 = Dense(1, activation='sigmoid')(fc1)\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    train_lis=[]\n",
    "    for f in os.listdir(train_faces_path):\n",
    "        for mid in os.listdir(train_faces_path+f):\n",
    "            if 'MID' in mid:\n",
    "                for image in os.listdir(train_faces_path+f+'/'+mid):\n",
    "                    train_lis.append(train_faces_path+f+'/'+mid+'/'+image)\n",
    "\n",
    "    val_lis=[]\n",
    "    for f in os.listdir(validate_faces_path):\n",
    "        for mid in os.listdir(validate_faces_path+f):\n",
    "            if 'MID' in mid:\n",
    "                for image in os.listdir(validate_faces_path+f+'/'+mid):\n",
    "                    val_lis.append(validate_faces_path+f+'/'+mid+'/'+image)\n",
    "\n",
    "    test_lis=[]\n",
    "    for f in os.listdir(test_faces_path):\n",
    "        for mid in os.listdir(test_faces_path+f):\n",
    "            if 'MID' in mid:\n",
    "                for image in os.listdir(test_faces_path+f+'/'+mid):\n",
    "                    test_lis.append(test_faces_path+f+'/'+mid+'/'+image)\n",
    "    \n",
    "    print(len(train_lis))\n",
    "    print(train_lis[0])\n",
    "\n",
    "    print(len(val_lis))\n",
    "    print(val_lis[0])\n",
    "\n",
    "    print(len(test_lis))\n",
    "    print(test_lis[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    train_dic={}\n",
    "    for image in tqdm(train_lis):\n",
    "        train_dic[image]=cal_feature(image, base_network)\n",
    "\n",
    "    val_dic={}\n",
    "    for image in tqdm(val_lis):\n",
    "        val_dic[image]=cal_feature(image, base_network)\n",
    "\n",
    "    test_dic={}\n",
    "    for image in tqdm(test_lis):\n",
    "        test_dic[image]=cal_feature(image, base_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    # Apply feature extraction to dataframe\n",
    "    train_new_pair['p1_feature'] = train_new_pair.apply(lambda x: train_dic[x['p1_path']], axis=1)\n",
    "    train_new_pair['p2_feature'] = train_new_pair.apply(lambda x: train_dic[x['p2_path']], axis=1)\n",
    "    val_new_pair['p1_feature'] = val_new_pair.apply(lambda x: val_dic[x['p1_path']], axis=1)\n",
    "    val_new_pair['p2_feature'] = val_new_pair.apply(lambda x: val_dic[x['p2_path']], axis=1)\n",
    "    test_new_pair['p1_feature'] = test_new_pair.apply(lambda x: test_dic[x['p1_path']], axis=1)\n",
    "    test_new_pair['p2_feature'] = test_new_pair.apply(lambda x: test_dic[x['p2_path']], axis=1)\n",
    "\n",
    "    train_new_pair['feature_distance'] = train_new_pair.apply(lambda x: fuse_features(x['p1_feature'], x['p2_feature'], fusion_type), axis=1)\n",
    "    val_new_pair['feature_distance'] = val_new_pair.apply(lambda x: fuse_features(x['p1_feature'], x['p2_feature'], fusion_type), axis=1)\n",
    "    test_new_pair['feature_distance'] = test_new_pair.apply(lambda x: fuse_features(x['p1_feature'], x['p2_feature'], fusion_type), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    tqdm.pandas(desc='Processing:')\n",
    "    train_new_pair['p1_feature']=train_new_pair.progress_apply(lambda x: train_dic[x['p1_path']], axis=1)\n",
    "    train_new_pair['p2_feature']=train_new_pair.progress_apply(lambda x: train_dic[x['p2_path']], axis=1)\n",
    "    train_new_pair['feature_distance']=train_new_pair.progress_apply(lambda x: np.abs(x['p1_feature']-x['p2_feature']), axis=1)\n",
    "\n",
    "    val_new_pair['p1_feature']=val_new_pair.progress_apply(lambda x: val_dic[x['p1_path']], axis=1)\n",
    "    val_new_pair['p2_feature']=val_new_pair.progress_apply(lambda x: val_dic[x['p2_path']], axis=1)\n",
    "    val_new_pair['feature_distance']=val_new_pair.progress_apply(lambda x: np.abs(x['p1_feature']-x['p2_feature']), axis=1)\n",
    "\n",
    "    test_new_pair['p1_feature']=test_new_pair.progress_apply(lambda x: test_dic[x['p1_path']], axis=1)\n",
    "    test_new_pair['p2_feature']=test_new_pair.progress_apply(lambda x: test_dic[x['p2_path']], axis=1)\n",
    "    test_new_pair['feature_distance']=test_new_pair.progress_apply(lambda x: np.abs(x['p1_feature']-x['p2_feature']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    print(train_new_pair)\n",
    "    print(val_new_pair)\n",
    "    print(test_new_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    # train_new_pair.to_csv(f'{fusion_type}_full_train_pairs.csv.gz', compression='gzip')\n",
    "    train_ptype_arr = train_new_pair['ptype'].values\n",
    "    train_distance = train_new_pair['feature_distance'].values\n",
    "\n",
    "    # val_new_pair.to_csv(f'{fusion_type}_full_val_pairs.csv.gz', compression='gzip')\n",
    "    val_ptype_arr = val_new_pair['ptype'].values\n",
    "    val_distance = val_new_pair['feature_distance'].values\n",
    "\n",
    "    # test_new_pair.to_csv(f'{fusion_type}_full_test_pairs.csv.gz', compression='gzip')\n",
    "    test_ptype_arr = test_new_pair['ptype'].values\n",
    "    test_distance = test_new_pair['feature_distance'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_distance_list(distance_list, list_name, chunk_size, expected_shape):\n",
    "    length = len(distance_list)\n",
    "    print(f\"Length of {list_name}: {length}\")\n",
    "    num_chunks = length // chunk_size\n",
    "    print(f\"Number of full chunks for {list_name}: {num_chunks}\")\n",
    "\n",
    "    result_list = []\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "        distance_slice = distance_list[start_idx:end_idx]\n",
    "        if len(distance_slice) == chunk_size:\n",
    "            dis_arr = np.vstack(distance_slice)\n",
    "            if dis_arr.shape == expected_shape:\n",
    "                result_list.append(dis_arr)\n",
    "            else:\n",
    "                print(f\"Skipping iteration {i} for {list_name} due to shape mismatch: {dis_arr.shape}\")\n",
    "        else:\n",
    "            print(f\"Skipping iteration {i} for {list_name} due to incorrect slice length: {len(distance_slice)}\")\n",
    "    \n",
    "    # Handle any remaining elements\n",
    "    remaining_start_idx = num_chunks * chunk_size\n",
    "    remaining_distance_slice = distance_list[remaining_start_idx:]\n",
    "    if len(remaining_distance_slice) > 0:\n",
    "        dis_arr = np.vstack(remaining_distance_slice)\n",
    "        if dis_arr.shape[1] == expected_shape[1]:  # Check only the number of columns\n",
    "            result_list.append(dis_arr)\n",
    "        else:\n",
    "            print(f\"Skipping remaining elements for {list_name} due to shape mismatch: {dis_arr.shape}\")\n",
    "    \n",
    "    if result_list:\n",
    "        combined_dis_arr = np.vstack(result_list)\n",
    "        print(f\"Final {list_name} shape: {combined_dis_arr.shape}\")\n",
    "        return combined_dis_arr\n",
    "    else:\n",
    "        print(f\"No valid arrays to concatenate for {list_name}\")\n",
    "        return None\n",
    "\n",
    "if calculate_features:\n",
    "    chunk_size = 4409  # Dynamic chunk size\n",
    "    expected_shape = (chunk_size, 2048 )\n",
    "\n",
    "    train_dis_arr = process_distance_list(train_distance, \"train_distance\", chunk_size, expected_shape)\n",
    "    val_dis_arr = process_distance_list(val_distance, \"val_distance\", chunk_size, expected_shape)\n",
    "    test_dis_arr = process_distance_list(test_distance, \"test_distance\", chunk_size, expected_shape)\n",
    "\n",
    "    # Print the shapes of the final arrays\n",
    "    if train_dis_arr is not None:\n",
    "        print(f\"train_dis_arr.shape: {train_dis_arr.shape}\")\n",
    "    if val_dis_arr is not None:\n",
    "        print(f\"val_dis_arr.shape: {val_dis_arr.shape}\")\n",
    "    if test_dis_arr is not None:\n",
    "        print(f\"test_dis_arr.shape: {test_dis_arr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    print(train_dis_arr.shape, train_ptype_arr.shape)\n",
    "    print(val_dis_arr.shape, val_ptype_arr.shape)\n",
    "    print(test_dis_arr.shape, test_ptype_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_features:\n",
    "    np.save(f'{fusion_type}_train_dis_arr.npy', train_dis_arr, allow_pickle=True)\n",
    "    np.save(f'{fusion_type}_train_ptype_arr.npy', train_ptype_arr, allow_pickle=True)\n",
    "\n",
    "    np.save(f'{fusion_type}_val_dis_arr.npy', val_dis_arr, allow_pickle=True)\n",
    "    np.save(f'{fusion_type}_val_ptype_arr.npy', val_ptype_arr, allow_pickle=True)\n",
    "\n",
    "\n",
    "    np.save(f'{fusion_type}_test_dis_arr.npy', test_dis_arr, allow_pickle=True)\n",
    "    np.save(f'{fusion_type}_test_ptype_arr.npy', test_ptype_arr, allow_pickle=True)\n",
    "    pass\n",
    "else:\n",
    "    # Load the saved files\n",
    "    if os.path.exists(f'{fusion_type}_train_dis_arr.npy'):\n",
    "        train_dis_arr = np.load(f'{fusion_type}_train_dis_arr.npy')\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{fusion_type}_train_dis_arr.npy not found.\")\n",
    "    \n",
    "    if os.path.exists(f'{fusion_type}_train_ptype_arr.npy'):\n",
    "        train_ptype_arr = np.load(f'{fusion_type}_train_ptype_arr.npy')\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{fusion_type}_train_ptype_arr.npy not found.\")\n",
    "    \n",
    "    if os.path.exists(f'{fusion_type}_val_dis_arr.npy'):\n",
    "        val_dis_arr = np.load(f'{fusion_type}_val_dis_arr.npy')\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{fusion_type}_val_dis_arr.npy not found.\")\n",
    "    \n",
    "    if os.path.exists(f'{fusion_type}_val_ptype_arr.npy'):\n",
    "        val_ptype_arr = np.load(f'{fusion_type}_val_ptype_arr.npy')\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{fusion_type}_val_ptype_arr.npy not found.\")\n",
    "    \n",
    "    if os.path.exists(f'{fusion_type}_test_dis_arr.npy'):\n",
    "        test_dis_arr = np.load(f'{fusion_type}_test_dis_arr.npy')\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{fusion_type}_test_dis_arr.npy not found.\")\n",
    "    \n",
    "    if os.path.exists(f'{fusion_type}_test_ptype_arr.npy'):\n",
    "        test_ptype_arr = np.load(f'{fusion_type}_test_ptype_arr.npy')\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{fusion_type}_test_ptype_arr.npy not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_label(label):\n",
    "    return{\n",
    "        'ms':0,\n",
    "        'fs':1,\n",
    "        'bb':2,\n",
    "        'sibs':3,\n",
    "        'fd':4,\n",
    "        'md':5,\n",
    "        'ss':6,\n",
    "        'gfgs':7,\n",
    "        'gfgd':8,\n",
    "        'gmgs':9,\n",
    "        'gmgd':10\n",
    "    }.get(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ptype_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_switched=np.array(list(map(switch_label,train_ptype_arr)))\n",
    "val_label_switched=np.array(list(map(switch_label,val_ptype_arr)))\n",
    "\n",
    "train_label_switched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train1=to_categorical(train_label_switched)\n",
    "y_val1=to_categorical(val_label_switched)\n",
    "y_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre2label(pred):\n",
    "    l=pred.tolist()\n",
    "    index=l.index(max(l))\n",
    "    return{\n",
    "        0:'ms',\n",
    "        1:'fs',\n",
    "        2:'bb',\n",
    "        3:'sibs',\n",
    "        4:'fd',\n",
    "        5:'md',\n",
    "        6:'ss',\n",
    "        7:'gfgs',\n",
    "        8:'gfgd',\n",
    "        9:'gmgs',\n",
    "        10:'gmgd'\n",
    "    }.get(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_pair['ptype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_switched=np.array(list(map(switch_label,train_ptype_arr)))\n",
    "val_label_switched=np.array(list(map(switch_label,val_ptype_arr)))\n",
    "train_label_switched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train1=to_categorical(train_label_switched)\n",
    "y_val1=to_categorical(val_label_switched)\n",
    "y_train1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'grid',  # 'grid', 'random', or 'bayes'\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.001, 0.002, 0.005]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'decay': {\n",
    "            'values': [1e-6, 1e-5, 1e-4]\n",
    "        },\n",
    "        'momentum': {\n",
    "            'values': [0.8, 0.9, 0.95]\n",
    "        },\n",
    "        'train_data': {\n",
    "            'values': [train_faces_path]\n",
    "        },\n",
    "        'fusion_type': {\n",
    "            'values': [fusion_type]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DSPRO2\")\n",
    "\n",
    "def train():\n",
    "    # Initialize a new W&B run\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=fusion_input_dim))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(y_train1.shape[1], activation='sigmoid'))\n",
    "\n",
    "    sgd = SGD(learning_rate=config.learning_rate, decay=config.decay, momentum=config.momentum, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define the EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model with the WandbCallback and EarlyStopping\n",
    "    model.fit(train_dis_arr, y_train1, validation_data=(val_dis_arr, y_val1), verbose=2, epochs=500,\n",
    "              batch_size=config.batch_size, callbacks=[WandbCallback(), early_stopping])\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train, count=1)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dis_arr)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_label=np.array(list(map(pre2label,preds)))\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_ptype_arr, preds_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame(columns=['true_label','predict_label','result'])\n",
    "df4['true_label']=val_ptype_arr\n",
    "df4['predict_label']=preds_label\n",
    "df4['result']=df4.apply(lambda x: x['true_label']==x['predict_label'], axis=1)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list=[]\n",
    "for kinship in df4['true_label'].unique():\n",
    "    subdf=df4[df4['true_label']==kinship]\n",
    "    accuracy=subdf['result'].sum()/len(subdf)\n",
    "    acc_list.append(accuracy)\n",
    "    print('Accuracy of {} is {}'.format(kinship,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=pd.DataFrame(columns=['kinship','accuracy'])\n",
    "df5['kinship']=df4['true_label'].unique()\n",
    "df5['accuracy']=acc_list\n",
    "df5['accuracy']=df5.apply(lambda x:round(x['accuracy'],4),axis=1)\n",
    "df5=df5.sort_values('accuracy',ascending=False).reset_index(drop=True)\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(df5, x='kinship', y='accuracy',text='accuracy',\n",
    "             hover_data=['accuracy'], color='accuracy',\n",
    "             height=600, title='Accuracy of each kinship')\n",
    "fig.update_traces( textposition='outside')\n",
    "fig.update_layout(\n",
    "    title=\"Accuracy of each kinship\",\n",
    "    xaxis_title=\"Kinship\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the weight of the model\n",
    "Make sure that you are already logged in to huggingface with `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f'{fusion_type}_kinship.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "repo_name = \"DSPRO2\"  \n",
    "username = \"Leo1212\"\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "# Save the model weights to the repository\n",
    "api.upload_file(\n",
    "    path_or_fileobj=f\"{fusion_type}_kinship.h5\",\n",
    "    path_in_repo=f\"{fusion_type}_kinship.h5\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
